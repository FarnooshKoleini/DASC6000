---
title: "WA07"
author: "Farnoosh Koleini"
date: '2022-12-09'
output: html_document
---
# Assignment Goal

The overarching goal of this assignment is to deepen your understanding of functions of random variables.

# Instructions

Please type your response to a question right below the question text. Compile this document to generate PDF output. Upload your PDF document to MS Teams. In the preamble above, change `V.N. Gudivada` to your name. 


# Questions

## CDF for a function of a continuous random variable

Let $X$ be a strictly continuous random variable with CDF $\mathbb{F}_{X}(x)$. Write an expression for the CDF of $Y=X^{4}$ in terms of $\mathbb{F}_{X}(x)$.

**Solution**: 

We need to find the CDF of Y in terms of CDF of X. 
$F_Y(y) = P(Y \leq y)$ This is a continuous distribution. We are trying to find the area under the curve based on the special values. $Y = X^4$ so $F_Y(y) = P(Y \leq y) = P(X^4 \leq y)$, and $X = +Y^{\frac {1}{4}}$ and $X = -Y^{\frac {1}{4}}$. 

So, $F_Y(y) = P(Y \leq y) = P(X^4 \leq y) = P(-Y^{\frac {1}{4}} < X < +Y^{\frac {1}{4}})$, basically what we have to do is, we need to subtract area of $+Y^{\frac {1}{4}}$ from $-Y^{\frac {1}{4}}$. 

$F_Y(y) = P(Y \leq y) = P(X^4 \leq y) = P(-Y^{\frac {1}{4}} < X < +Y^{\frac {1}{4}}) = P(X\leq Y^{\frac {1}{4}}) - P(X\leq Y^{\frac {-1}{4}}) = F_X(Y^{\frac {1}{4}}) - F_X(-Y^{\frac {-1}{4}})$ where A = {all real numbers} and B is when y values are greater or equal to 0. 

<div style="text-align: right"> $\blacksquare$ </div> 

*** 
    
    
## PMF of iid Bernoulli random variables

Let $X_{1}$ and $X_{2}$ be independent and identically distributed Bernoulli$(p)$ random variables. What is the PMF of $Y = X_{1}-X_{2}$?

**Solution**: 

We have Bernoulli random variables X1 and X2. Problem says these two variables are two independent and identically distributed Bernoulli random variables. It means they have the same parameter P and they are independent. Another thing is these two values can only get 2 possible solutions 0 or 1. All the possibilities for X1 and X2 would be 4, (0,0), (0,1), (1,0), (1,1). And we can easily calculate the $Y = X_{1}-X_{2}$ for all of these possibilities. These values would be 0, -1, 1, and 0 respectively. Our random variable A has a support A = {0,1} and B = {-1, 0, 1}. Now it is time to associate the probabilities with these values. First case would be (1-p)(1-p) and second one (1-p)(p) and third case would be (p)(1-p) and last case would be (p)(p).  So $p(1-p)$, $p^2 (1- p)^2$, and $p(1-p)$ when $f_Y(y) = -1, 0, 1$ respectively.

<div style="text-align: right"> $\blacksquare$ </div> 

*** 


## Independent uniform random variables

Let $X_{1}$ and $X_{2}$ be independent $U(0,1)$ random variables. Let $Y_{1}=X_{1}+X_{2}$ and $Y_{2}=$ $X_{1} /\left(X_{1}+X_{2}\right)$.

(a) Find the joint probability density function of $Y_{1}$ and $Y_{2}$.

**Solution**: 

First we need to go through the 6 steps process to solve this problem. 1) We need to find joint density which is the product of marginal densities. We also need to specify support, 0< x1< 1, and 0< x2< 1. We also can simplify this further.

$f_{x_{1},x_{2}} (x_{1},x_{2}) = f_{x}(x_{1}) f_{2}(x_{2}) = 1.1$

Our script A is {0< x1< 1, and 0< x2< 1}. We need to know is this a one to one transformation? We need to figure this out in Studio. We can say that y1 > 0 and y2>0 and y2< 1. We can write the equations of the lines, $y2 < \frac{1}{y1}$ and the other line $y2 > \frac{y1-1}{y1}$. Our transformation equation is: $y_1 = x_1 +x_2$ and $y2 = \frac{x_1}{x_1 + x_2}$. $x_1 = y_1 y_2$ and $x_2 = y_1 - y_1 y_2$ Now it is time to get Jacobin matrix. and after taking the determinate of that matrix. The result would be $|-y_1| = y_1$. We are going to write $f_{Y_1,Y_2} (y_1, y_2) = f_{X_1, X_2}(g_1^{-1} (y1,y2), g_2^{-1}(y_{1},y_{2})|J|$, it is going to be equal to $1 \times y_1 = y_1$ 
    
(b) Find the probability density function of $Y_{1}$.

**Solution**: 

When y1 goes from one to 2 there is the only variation there. So, $\int_0 ^1 y_1 dy_2 = y_1$ where $0 < y_1 < 1$ and also $\int^{1/y1}_{\frac{y_1 - 1}{y1}} y_1 dy_2 = 2-y_1$, where $1 < y_1 < 2$. 

(c) Find the probability density function of $Y_{2}$.

**Solution**: 

We can repeat the same thing for $Y_{2}$. We need to do it in two parts. 

$f_Y2(y3) = \int_0 ^{\frac {1}{y2 - 1}} y_1 dy_1 = \frac{1}{2(y2 - 1)^2$ where $0 < y_2 < 0.5$. 
$f_Y2(y3) = \int_0 ^{\frac {1}{y2}} y_1 dy_1$ where $0.5 < y2 < 1$. 


<div style="text-align: right"> $\blacksquare$ </div> 

*** 


## Finding distribution names and parameter values for functions of random variables

Let $X_{1} \sim N(0,1), X_{2} \sim \chi^{2}(1)$, and $X_{3} \sim \chi^{2}(n)$ be mutually independent random variables. Find the probability distribution (name and parameter values) of (No mathematics is required on this problem; simply write down the solution):
    
(a) $-7 X_{1}$

**Solution**: 

No mathematics is required, simply write down the solution. We have a fact which is the linear combination of normal random variables is also normally distributed. So $-7 X_{1}$ has a normal distribution that mean value is 0 and variance would be square of 7. Therefore when we have a standard normal distribution, if we multiply that with an integer. Mean is gonna be 0 and variance would be the square of standard deviation. 
$-7 X_{1}$ ~ $N (0,7^2)$

(b) $X_{1}^{2}+X_{3}$

**Solution**: 

We have another fact that square of the normal random variable ki squared with one degree of freedom. Second one is X3 with ki square. We have another fact which says the sum of independent ki squares is ki squared of the summation of those two normal random variables. $X_{1}^{2}+X_{3}$ ~ $ /X^2 (1+n) $

(c) $X_{3} /\left(n X_{1}^{2}\right)$

**Solution**: 

We have the standard normal that we are squaring it. When we square it like the b and multiply with n. 

$\frac {\frac {X^2(K1)}{K1}}{\frac{X^2(K2)}{K2}}$, we can use this result to say $X_{3} /\left(n X_{1}^{2}\right) = \frac {X_3/n}{X_1^2/1}$, so this is going to be distribution with $F(n,1)$.

(d) $X_{1} / \sqrt{X_{2}}$

**Solution**: 

We have the fact that says we have std normal random variable. $\frac {N(0,1) \sim t} {\sqrt \frac {X^2(k)}{k}}$ ~ $t(k)$. So now we can answer the d question which is  $X_{1} / \sqrt{X_{2}}$ ~ $t(k)$.

(e) $\sqrt{n} X_{1} / \sqrt{X_{3}}$

**Solution**: 

The fact we are going to use here is the ratio of the standard normal distribution. And the square root of the independent square distribution (k) divided by k would be t(k). $\frac {N(0,1) \sim t} {\sqrt \frac {X^2(k)}{k}}$. The solution would be:
$X_{1} / \sqrt{X_{3}/n}$
   
<div style="text-align: right"> $\blacksquare$ </div> 

*** 


## PDF of a function of a continuous random variable

A farmer would like to build a rectangular pig pen along a long brick wall using $l$ feet of fencing for the three sides. Let $X$ be the length of the sides of the pen that are perpendicular to the brick wall. If $X$ has PDF

$$
f(x)=\frac{8 x}{l^{2}}, \quad 0<x<l / 2,
$$
    
find the PDF of the area of the pen.

**Solution**: 

$Y = (l-2X)X$, the area is going to be a maximum, $Y = (X - 2X^2)$ and $dy/dx = l - 4X$ and $X = l/4$. And now it is time to calculate Y which is $Y = \frac {l^2}{8}$ also we know that A would be 0 < x < l/2. The maximum area was $l^2/8$. Now we need to think about transformation, $Y = X(l - 2x) = Xl - 2 X^2 = 2X^2 - lX + Y= 0$

$X = \frac{-(-l) + \sqrt {l^2 - 8Y}}{4}$ or $X = \frac{-(-l) - \sqrt {l^2 - 8Y}}{4}$after simplification. It is going to be $X =\frac {l + \sqrt{l^2 - 8Y}}{4}$ or $X =\frac {l - \sqrt{l^2 - 8Y}}{4}$. 

$dx/dy = \frac {1}{\sqrt {l^2 - 8y}}$ or $dx/dy = \frac {-1}{\sqrt {l^2 - 8y}}$. We are ready to write $f_Y(y) = \frac {2l - 2 \sqrt {l^2 - 8y}}{l^2}$ After simplification, the solution would be $\frac {4}{l\sqrt{l^2 - 8y}}$.  where $0 < y < l^2/8$. 


<div style="text-align: right"> $\blacksquare$ </div> 

*** 



## Find PDF by using the transformation technique

Let the random variables $X_{1}$ and $X_{2}$ be defined by the joint PDF:

$$
f_{X_{1}, X_{2}}\left(x_{1}, x_{2}\right)=\frac{1}{4} \quad 0<x_{1}<1,0<x_{2}<4
$$
    
Use the transformation technique to find the PDF of $Y_{1}=X_{1} / X_{2}$ using the dummy transformation $Y_{2}=X_{2}$. Make sure to show the region $\mathcal{B}$, the inverse transformation, and the Jacobian of the inverse transformation.


**Solution**: 

We need to use the transformation technique to find the pdf of Y1 which is a ratio given in the question. $Y1 = g1(x1,x2) = x1/x2$ and $Y_{1}=X_{1} / X_{2}$, where support for A is $0<x_{1}<1,0<x_{2}<4$.
If we want to find the support for B, it is gonna be: $y1> 0, 0<y2<4, y2 < y1$. We know that X1 is given as $X1 = g^{-1} (y1, y2) = y1y2$. After jacobian and finding the determinant value. It is going to be y2. We need to write the joint density $f_{Y1,Y2}(y1,y2)= 1/4 |y_2| = \frac {y_2}{4}$. and the support for this would be (y1,y2) member of B. Now we need to find PDF of this which is going to be the integration from 0 to 4 for y1 ( = 2) and from 0 to 1/y1 for y2 ($= \frac {1}{8y1^2}$). Also, after all the calculations, the final solution after adding this two integration values would be 1. 

<div style="text-align: right"> $\blacksquare$ </div> 

*** 

## Order Statistics

Let $X_{1}$ and $X_{2}$ have joint PDF

$$
f\left(x_{1}, x_{2}\right)=\frac{x_{1}+5 x_{2}}{3}, \quad 0<x_{1}<1,0<x_{2}<1
$$

(a) Find $\mathbb{V}\left[X_{(2)}\right]$, where $X_{(1)}$ and $X_{(2)}$ are the order statistics, that is, $X_{(1)}=\min \left\{X_{1}, X_{2}\right\}$ and $X_{(2)}=\max \left\{X_{1}, X_{2}\right\}$.

**Solution**: 

The biggest issue here for order statistics is notation. It is kind of confusing! We have the joint distribution here in this example. We need to find the variance of x2 where X1 and X2 are in the order statistics. x1 is the minimum of the two values and x2 is the maximum of the two values. Once we have the marginal distributions, for X2, we can find the variance X2 as Expected value of X_2. The support for A would be 0<x1< 1 and 0<x2<1. And support for B would be all the pairs x(1) and x(2) such that 0 < x(1) < x(2) < 1. This is a two to one transformation. We have something like doing transformation. After finding the determinant of jacobian we can find the joint distribution after transformation would be $2(x(1) + x(2))$ where 0 <x(1) < x(2) < 1. Now we need to find the marginal distribution $f_{X(2)} (x(2)) = \int_0 ^{x(2)} 2(x(1) + x(2))dx(1) = 3 x(2)^2$. Now we can find the $E[X(2)] = \int_0^1 x. 3(x^2) dx(2) = 3/4$,

$E[X_(2)^2] = 3/5$, Now we are ready to compute variance of X(2) which is $E[X(2)^2] - E[X(2)]^2 = 3/5 - 9/16 = 3/80 = 0.0375$

(b) Support your solution to part (a) using Monte Carlo simulation.

**Solution**:

<div style="text-align: right"> $\blacksquare$ </div> 

*** 


## Order Statistics 

Susan draws five billiard balls at random and without replacement from a bag containing billiard balls numbered $1,2, \ldots, 15$. Let $X_{1}, X_{2}, \ldots, X_{5}$ denote the numbers drawn, and $X_{(1)}, X_{(2)}, \ldots, X_{(5)}$ denote the corresponding order statistics.

(a) Find the probability mass function of the third order statistic, $X_{(3)}$.

**Solution**: 

$\frac {f_{X(3)}(x(3)) = (x(3) -1 )(x(3) -2 )(15 -x(3))(14 - x(3)}{12012}$, and  $x(3) = 3, 4, ..., 13$. 


(b) Find the joint PMF of $X_{(3)}$ and $X_{(4)}$.

**Solution**: 

The probability mass function would be $f_{X(3), X(4)} (x(3),x(4)) = \frac {(x(3) -1 )(x(3) -2) (15 - x(4))}{60006}$ and $x(3) = 3, 4, ..., 13$, $x(4) = 4, 5, ..., 14$. 


<div style="text-align: right"> $\blacksquare$ </div> 

*** 

## MGF for the sum of gamma random variables

Let $X_{1} \sim \operatorname{gamma}\left(\lambda_{1}, \kappa_{1}\right), X_{2} \sim \operatorname{gamma}\left(\lambda_{2}, \kappa_{2}\right), X_{3} \sim \operatorname{gamma}\left(\lambda_{3}, \kappa_{3}\right)$, be mutually independent random variables. Find the moment generating function of

$$
Y=a_{1} X_{1}+a_{2} X_{2}+a_{3} X_{3}
$$
    
for real, positive constants $a_{1}, a_{2}, a_{3}$.


**Solution**: 

$m_X(t) = (\frac {\lambda}{\lambda \times t})^k$ where $t < \lambda$. Also $m_Y(t) = M_{a1, X1} (t) \times M_{a2,X2}(t) \times M_{a3,X3}(t)$. After putting all of them in the first formula. And t is the same for all of them which is less than minimum of lambda 1/a1, lambda 2/a2, lambda 3/a3. 

   
<div style="text-align: right"> $\blacksquare$ </div> 

*** 

## MGF for the sum of geometric random variables

Let $X_{1}, X_{2}, \ldots, X_{n}$ be mutually independent and identically distributed geometric $(p)$ random variables. Use the moment generating function technique to find the distribution of $Y=X_{1}+X_{2}+\cdots+X_{n}$.


**Solution**: 

We are going to come up with moment generating function. It is same as some distribution we have seen before. $M_Xi(t) = \frac {P}{1-(1-P) e^t}$ and where t< -ln(1-p). and i = 1, 2, ..., n. Because all of them are mutually iid, the moment generating function for y is going to be simply the product of  $M_Xi(t) = \frac {P}{1-(1-P) e^t}$. The answer would be $(\frac {P}{1-(1-P) e^t})^n$ and where t< -ln(1-p). Negative binomial distribution. 

<div style="text-align: right"> $\blacksquare$ </div> 

*** 